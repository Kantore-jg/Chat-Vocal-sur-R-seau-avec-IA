import json
import wave
import io
import pyttsx3
from vosk import Model, KaldiRecognizer
import pyaudio

class LocalAI:
    def __init__(self, vosk_model_path="model"):
        """
        Initialiser l'IA locale
        
        Args:
            vosk_model_path: Chemin vers le mod√®le Vosk
                             T√©l√©charger depuis: https://alphacephei.com/vosk/models
                             Recommand√©: vosk-model-small-fr-0.22 pour fran√ßais
        """
        print("ü§ñ Initialisation de l'IA locale...")
        
        # Speech-to-Text (Vosk)
        try:
            self.vosk_model = Model(vosk_model_path)
            print(f"‚úÖ Mod√®le Vosk charg√© depuis: {vosk_model_path}")
        except Exception as e:
            print(f"‚ùå Erreur chargement Vosk: {e}")
            print("üí° T√©l√©chargez un mod√®le sur https://alphacephei.com/vosk/models")
            self.vosk_model = None
        
        # Text-to-Speech (pyttsx3)
        try:
            self.tts_engine = pyttsx3.init()
            
            # Configuration TTS
            self.tts_engine.setProperty('rate', 150)  # Vitesse
            self.tts_engine.setProperty('volume', 0.9)  # Volume
            
            # Essayer de d√©finir une voix fran√ßaise si disponible
            voices = self.tts_engine.getProperty('voices')
            for voice in voices:
                if 'french' in voice.name.lower() or 'fr' in voice.id.lower():
                    self.tts_engine.setProperty('voice', voice.id)
                    break
            
            print("‚úÖ Moteur TTS initialis√©")
        except Exception as e:
            print(f"‚ùå Erreur initialisation TTS: {e}")
            self.tts_engine = None
        
        # Base de connaissances simple pour l'assistant
        self.responses = {
            "bonjour": "Bonjour ! Comment puis-je vous aider ?",
            "salut": "Salut ! Je suis l√† pour discuter !",
            "comment √ßa va": "Je vais bien, merci ! Et vous ?",
            "au revoir": "Au revoir ! √Ä bient√¥t !",
            "merci": "De rien, avec plaisir !",
            "aide": "Je peux transcrire vos messages vocaux et r√©pondre √† vos questions !",
            "qui es-tu": "Je suis un assistant IA local int√©gr√© au chat vocal !",
            "heure": None,  # Sera g√©r√© dynamiquement
            "date": None,   # Sera g√©r√© dynamiquement
        }
    
    def speech_to_text_from_wav(self, audio_data):
        """
        Convertir audio WAV en texte avec Vosk
        
        Args:
            audio_data: bytes - donn√©es audio au format WAV
            
        Returns:
            str: Texte transcrit ou None si erreur
        """
        if not self.vosk_model:
            print("‚ùå Mod√®le Vosk non charg√©")
            return None
        
        try:
            # Lire le WAV depuis les bytes
            audio_buffer = io.BytesIO(audio_data)
            with wave.open(audio_buffer, 'rb') as wf:
                # V√©rifier le format
                if wf.getnchannels() != 1:
                    print("‚ö†Ô∏è  Audio doit √™tre mono")
                    return None
                
                # Cr√©er le recognizer
                recognizer = KaldiRecognizer(self.vosk_model, wf.getframerate())
                recognizer.SetWords(True)
                
                # Traiter l'audio
                text_parts = []
                while True:
                    data = wf.readframes(4000)
                    if len(data) == 0:
                        break
                    
                    if recognizer.AcceptWaveform(data):
                        result = json.loads(recognizer.Result())
                        if 'text' in result and result['text']:
                            text_parts.append(result['text'])
                
                # R√©cup√©rer le dernier morceau
                final_result = json.loads(recognizer.FinalResult())
                if 'text' in final_result and final_result['text']:
                    text_parts.append(final_result['text'])
                
                # Combiner tous les morceaux
                full_text = ' '.join(text_parts).strip()
                
                if full_text:
                    print(f"üìù Transcription: \"{full_text}\"")
                    return full_text
                else:
                    print("‚ö†Ô∏è  Aucun texte d√©tect√©")
                    return None
                    
        except Exception as e:
            print(f"‚ùå Erreur transcription: {e}")
            return None
    
    def speech_to_text_live(self, duration=5):
        """
        Enregistrer et transcrire en direct depuis le micro
        
        Args:
            duration: Dur√©e d'enregistrement en secondes
            
        Returns:
            str: Texte transcrit
        """
        if not self.vosk_model:
            print("‚ùå Mod√®le Vosk non charg√©")
            return None
        
        try:
            print(f"üé§ Enregistrement en cours ({duration}s)...")
            
            audio = pyaudio.PyAudio()
            stream = audio.open(
                format=pyaudio.paInt16,
                channels=1,
                rate=16000,
                input=True,
                frames_per_buffer=8000
            )
            
            recognizer = KaldiRecognizer(self.vosk_model, 16000)
            recognizer.SetWords(True)
            
            text_parts = []
            frames_count = int(16000 / 8000 * duration)
            
            for _ in range(frames_count):
                data = stream.read(8000, exception_on_overflow=False)
                
                if recognizer.AcceptWaveform(data):
                    result = json.loads(recognizer.Result())
                    if 'text' in result and result['text']:
                        text_parts.append(result['text'])
            
            # R√©sultat final
            final_result = json.loads(recognizer.FinalResult())
            if 'text' in final_result and final_result['text']:
                text_parts.append(final_result['text'])
            
            stream.stop_stream()
            stream.close()
            audio.terminate()
            
            full_text = ' '.join(text_parts).strip()
            
            if full_text:
                print(f"üìù Transcription: \"{full_text}\"")
                return full_text
            else:
                print("‚ö†Ô∏è  Aucun texte d√©tect√©")
                return None
                
        except Exception as e:
            print(f"‚ùå Erreur enregistrement live: {e}")
            return None
    
    def text_to_speech(self, text, save_to_file=None):
        """
        Convertir du texte en parole
        
        Args:
            text: Texte √† synth√©tiser
            save_to_file: Optionnel - Chemin pour sauvegarder l'audio
            
        Returns:
            bool: True si succ√®s
        """
        if not self.tts_engine:
            print("‚ùå Moteur TTS non initialis√©")
            return False
        
        try:
            print(f"üó£Ô∏è  Synth√®se vocale: \"{text}\"")
            
            if save_to_file:
                self.tts_engine.save_to_file(text, save_to_file)
                self.tts_engine.runAndWait()
                print(f"üíæ Audio sauvegard√©: {save_to_file}")
            else:
                self.tts_engine.say(text)
                self.tts_engine.runAndWait()
            
            return True
            
        except Exception as e:
            print(f"‚ùå Erreur TTS: {e}")
            return False
    
    def get_ai_response(self, user_input):
        """
        G√©n√©rer une r√©ponse IA basique
        
        Args:
            user_input: Message de l'utilisateur
            
        Returns:
            str: R√©ponse de l'IA ou None
        """
        if not user_input:
            return None
        
        user_input_lower = user_input.lower().strip()
        
        # R√©ponses dynamiques
        if "heure" in user_input_lower:
            from datetime import datetime
            return f"Il est {datetime.now().strftime('%H:%M')}"
        
        if "date" in user_input_lower:
            from datetime import datetime
            return f"Nous sommes le {datetime.now().strftime('%d/%m/%Y')}"
        
        # R√©ponses pr√©d√©finies
        for keyword, response in self.responses.items():
            if keyword in user_input_lower:
                return response
        
        # R√©ponse par d√©faut
        if len(user_input.split()) > 3:
            return "Int√©ressant ! Pouvez-vous m'en dire plus ?"
        
        return None
    
    def process_audio_message(self, audio_data, auto_respond=False):
        """
        Pipeline complet: Audio -> Texte -> (Optionnel) R√©ponse
        
        Args:
            audio_data: bytes - Audio WAV
            auto_respond: bool - G√©n√©rer une r√©ponse automatique
            
        Returns:
            dict: {'transcription': str, 'response': str, 'response_audio': bytes}
        """
        result = {
            'transcription': None,
            'response': None,
            'response_audio': None
        }
        
        # 1. Speech-to-Text
        transcription = self.speech_to_text_from_wav(audio_data)
        result['transcription'] = transcription
        
        if not transcription:
            return result
        
        # 2. G√©n√©rer r√©ponse si demand√©
        if auto_respond:
            response = self.get_ai_response(transcription)
            result['response'] = response
            
            # 3. Text-to-Speech de la r√©ponse
            if response:
                # Cr√©er un fichier temporaire pour la r√©ponse
                import tempfile
                temp_file = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)
                temp_path = temp_file.name
                temp_file.close()
                
                if self.text_to_speech(response, save_to_file=temp_path):
                    # Lire le fichier audio g√©n√©r√©
                    try:
                        with open(temp_path, 'rb') as f:
                            result['response_audio'] = f.read()
                        
                        import os
                        os.remove(temp_path)
                    except Exception as e:
                        print(f"‚ùå Erreur lecture audio r√©ponse: {e}")
        
        return result


# Fonction utilitaire pour tester le module
def test_ai():
    """Tester les fonctionnalit√©s de l'IA"""
    print("üß™ TEST DU MODULE IA LOCALE")
    print("=" * 60)
    
    # Initialiser l'IA
    ai = LocalAI(vosk_model_path="model")
    
    # Test TTS
    print("\n1Ô∏è‚É£  Test Text-to-Speech:")
    ai.text_to_speech("Bonjour ! Je suis l'intelligence artificielle du chat vocal.")
    
    # Test r√©ponses
    print("\n2Ô∏è‚É£  Test g√©n√©ration de r√©ponses:")
    test_inputs = [
        "Bonjour",
        "Quelle heure est-il ?",
        "Qui es-tu ?",
        "Au revoir"
    ]
    
    for user_input in test_inputs:
        response = ai.get_ai_response(user_input)
        print(f"  üë§ User: {user_input}")
        print(f"  ü§ñ AI: {response}\n")
    
    # Test Speech-to-Text live (optionnel)
    print("\n3Ô∏è‚É£  Test Speech-to-Text (Appuyez sur Entr√©e pour enregistrer 5s)")
    input()
    ai.speech_to_text_live(duration=5)
    
    print("\n‚úÖ Tests termin√©s!")


if __name__ == "__main__":
    test_ai()